# -*- coding: utf-8 -*-
"""Grader Bias Calculation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xG_pifSOnhxjmUpSiIpFUwlp4Nuwl5rV

Grader Bias Calculation
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as stats
from google.colab import files
import statsmodels.api as sm

from google.colab import drive
drive.mount('/content/drive')
# Step 2: List files in your Google Drive to find your Excel file
# !ls /content/drive/MyDrive/

MDGrade = '/content/drive/MyDrive/LLM_Chatbot/NPJ_In-Basket/Round4_InBasket_Grading_MDs.xlsx'  # Replace with your actual path
MDGrade = pd.read_excel(MDGrade, engine='openpyxl')

# Completeness comparison
MDGrade['CompletenessAverage'] = MDGrade[['Dr.HCompleteness', ' DR.BCompleteness', 'LLMcompleteness']].mean(axis=1)

# Correctness comparison
MDGrade['CorrectnessAverage'] = MDGrade[['Dr.HCorrectness', ' Dr.BCorrectness', 'LLMcorrectness']].mean(axis=1)

# Clarity comparison
MDGrade['ClarityAverage'] = MDGrade[[' Dr.HClarity', 'Dr.BClarity', 'LLMclarity']].mean(axis=1)

# Empathy comparison
MDGrade['EmpathyAverage'] = MDGrade[['Dr.HEmpathy', 'Dr.BEmpathy', 'LLMempathy']].mean(axis=1)

# Completeness correlation
completeness_corr = MDGrade[['Dr.HCompleteness', ' DR.BCompleteness', 'LLMcompleteness']].corr()

# Correctness correlation
correctness_corr = MDGrade[['Dr.HCorrectness', ' Dr.BCorrectness', 'LLMcorrectness']].corr()

# Clarity correlation
clarity_corr = MDGrade[[' Dr.HClarity', 'Dr.BClarity', 'LLMclarity']].corr()

# Empathy correlation
empathy_corr = MDGrade[['Dr.HEmpathy', 'Dr.BEmpathy', 'LLMempathy']].corr()

print(completeness_corr, correctness_corr, clarity_corr, empathy_corr)

# Rename the columns to indicate the category (changing Dr. H to Dr. 1, and Dr. B to Dr. 2)
completeness_corr.columns = ['Dr. 1 Completeness', 'Dr. 2 Completeness']
correctness_corr.columns = ['Dr. 1 Correctness', 'Dr. 2 Correctness']
clarity_corr.columns = ['Dr. 1 Clarity', 'Dr. 2 Clarity']
empathy_corr.columns = ['Dr. 1 Empathy', 'Dr. 2 Empathy']

# Combine the correlation matrices into a single DataFrame
combined_corr = pd.concat([completeness_corr, correctness_corr, clarity_corr, empathy_corr], axis=1)

# Rename the index similarly for readability
combined_corr.index = ['Dr. 1 Completeness', 'Dr. 2 Completeness',
                       'Dr. 1 Correctness', 'Dr. 2 Correctness',
                       'Dr. 1 Clarity', 'Dr. 2 Clarity',
                       'Dr. 1 Empathy', 'Dr. 2 Empathy']

# Plot the combined heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(combined_corr, annot=True, cmap="coolwarm", vmin=-1, vmax=1, annot_kws={"color": "white"})  # Set white text
plt.rcParams['figure.dpi'] = 280

# Rotate the x-axis labels by 45 degrees
plt.xticks(rotation=30, ha='right')
# Display the heatmap
# plt.title("Combined Heatmap of Graders' Correlation Across Categories (Dr. 1 & Dr. 2)")
plt.show()

from scipy.stats import f_oneway
# Remove rows with NaN values
MDGrade_cleaned = MDGrade.dropna()

# Assuming we cleaned the data
f_value_completeness, p_value_completeness = f_oneway(MDGrade_cleaned['Dr.HCompleteness'], MDGrade_cleaned[' DR.BCompleteness'], MDGrade_cleaned['LLMcompleteness'])
f_value_correctness, p_value_correctness = f_oneway(MDGrade_cleaned['Dr.HCorrectness'], MDGrade_cleaned[' Dr.BCorrectness'], MDGrade_cleaned['LLMcorrectness'])
f_value_clarity, p_value_clarity = f_oneway(MDGrade_cleaned[' Dr.HClarity'], MDGrade_cleaned['Dr.BClarity'], MDGrade_cleaned['LLMclarity'])
f_value_empathy, p_value_empathy = f_oneway(MDGrade_cleaned['Dr.HEmpathy'], MDGrade_cleaned['Dr.BEmpathy'], MDGrade_cleaned['LLMempathy'])

# After cleaning the data
f_value_completeness, p_value_completeness = f_oneway(MDGrade_cleaned['Dr.HCompleteness'], MDGrade_cleaned[' DR.BCompleteness'], MDGrade_cleaned['LLMcompleteness'])
f_value_correctness, p_value_correctness = f_oneway(MDGrade_cleaned['Dr.HCorrectness'], MDGrade_cleaned[' Dr.BCorrectness'], MDGrade_cleaned['LLMcorrectness'])
f_value_clarity, p_value_clarity = f_oneway(MDGrade_cleaned[' Dr.HClarity'], MDGrade_cleaned['Dr.BClarity'], MDGrade_cleaned['LLMclarity'])
f_value_empathy, p_value_empathy = f_oneway(MDGrade_cleaned['Dr.HEmpathy'], MDGrade_cleaned['Dr.BEmpathy'], MDGrade_cleaned['LLMempathy'])

# Step 4: Output the F-values for each category
print("F-values:")
print(f"Completeness: F-value = {f_value_completeness}, p-value = {p_value_completeness}")
print(f"Correctness: F-value = {f_value_correctness}, p-value = {p_value_correctness}")
print(f"Clarity: F-value = {f_value_clarity}, p-value = {p_value_clarity}")
print(f"Empathy: F-value = {f_value_empathy}, p-value = {p_value_empathy}")

import numpy as np
from scipy.stats import pearsonr

# Assuming your DataFrame is named MDGrade
# Define the pairs for each category
categories = {
    'Completeness': ['Dr.HCompleteness', ' DR.BCompleteness'],
    'Correctness': ['Dr.HCorrectness', ' Dr.BCorrectness'],
    'Clarity': [' Dr.HClarity', 'Dr.BClarity'],
    'Empathy': ['Dr.HEmpathy', 'Dr.BEmpathy']
}

# Dictionary to store the Pearson correlation results
pearson_results = {}

# Calculate Pearson correlation for each category
for category, (col_h, col_b) in categories.items():
    # Filter out NaNs and inf values
    filtered_data = MDGrade[[col_h, col_b]].replace([np.inf, -np.inf], np.nan).dropna()

    grader1 = filtered_data[col_h].values
    grader2 = filtered_data[col_b].values

    # Calculate Pearson correlation coefficient
    pearson_corr, _ = pearsonr(grader1, grader2)

    # Store the result
    pearson_results[category] = pearson_corr

# Display the results
for category, corr in pearson_results.items():
    print(f"{category} Pearson correlation coefficient: {corr}")

# Dictionary to store the ICC results
icc_results = {}

# Calculate ICC for each category
for category, (col_h, col_b) in categories.items():
    # Prepare the data in the required format
    data = pd.DataFrame({
        'Target': range(len(MDGrade)),
        'Grader1': MDGrade[col_h],
        'Grader2': MDGrade[col_b]
    })

    # Melt the DataFrame to long format
    data_long = data.melt(id_vars=['Target'], var_name='Rater', value_name='Score')

    # Calculate the ICC, omitting rows with NaN values
    icc = pg.intraclass_corr(data=data_long, targets='Target', raters='Rater', ratings='Score', nan_policy='omit')

    # Extract the ICC value (ICC2k is often used for absolute agreement)
    icc_value = icc.loc[icc['Type'] == 'ICC2k', 'ICC'].values[0]

    # Store the result
    icc_results[category] = icc_value

# Display the ICC results
for category, icc_value in icc_results.items():
    print(f"{category} ICC: {icc_value}")

import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Dictionary to store the linear regression results
regression_results = {}

# Perform linear regression for each category
for category, (col_h, col_b) in categories.items():
    # Drop rows with NaN values
    filtered_data = MDGrade[[col_h, col_b]].dropna()

    # Reshape the data for sklearn
    X = filtered_data[col_h].values.reshape(-1, 1)  # Predictor (Dr. H)
    y = filtered_data[col_b].values  # Response (Dr. B)

    # Perform linear regression
    model = LinearRegression()
    model.fit(X, y)

    # Make predictions
    y_pred = model.predict(X)

    # Calculate R-squared
    r2 = r2_score(y, y_pred)

    # Store the coefficients, intercept, and R-squared
    regression_results[category] = {
        'Coefficient': model.coef_[0],
        'Intercept': model.intercept_,
        'R-squared': r2
    }

# Display the regression results
for category, results in regression_results.items():
    print(f"{category} Linear Regression:")
    print(f"  Coefficient: {results['Coefficient']}")
    print(f"  Intercept: {results['Intercept']}")
    print(f"  R-squared: {results['R-squared']}\n")

import pandas as pd
from sklearn.metrics import cohen_kappa_score
# Dictionary to store the Cohen's Kappa results
kappa_results = {}

# Calculate Cohen's Kappa score for each category
for category, (col_h, col_b) in categories.items():
    # Drop rows with NaN values
    filtered_data = MDGrade[[col_h, col_b]].dropna()

    # Calculate Cohen's Kappa score
    kappa_score = cohen_kappa_score(filtered_data[col_h], filtered_data[col_b])

    # Store the result
    kappa_results[category] = kappa_score

# Display the Cohen's Kappa results
for category, kappa_score in kappa_results.items():
    print(f"{category} Cohen's Kappa score: {kappa_score}")

import numpy as np
import matplotlib.pyplot as plt

# Assuming MDGrade is your DataFrame and contains the relevant columns

# Define the categories and corresponding columns
categories = {
    'Completeness': ['Dr.HCompleteness', ' DR.BCompleteness'],
    'Correctness': ['Dr.HCorrectness', ' Dr.BCorrectness'],
    'Clarity': [' Dr.HClarity', 'Dr.BClarity'],
    'Empathy': ['Dr.HEmpathy', 'Dr.BEmpathy']
}

# Calculate the absolute differences between Dr. H and Dr. B
values_diff = np.abs(np.array([0.4, 0.5, 0.2, 0.3]))  # Replace with your calculated values

# Combine the values with the first element to close the radar chart
values_diff = np.concatenate((values_diff, [values_diff[0]]))

# Calculate the angle for each category on the radar chart
angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
angles += angles[:1]  # Ensure the chart is circular

# Create the radar chart
fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))

# Plot the absolute differences
ax.plot(angles, values_diff, linewidth=2, linestyle='solid', color='darkorange', label='Absolute Difference (|B - H|)')
ax.fill(angles, values_diff, color='orange', alpha=0.4)

# Add the category labels to the chart
ax.set_xticks(angles[:-1])
ax.set_xticklabels(categories.keys(), fontsize=26, color='darkblue')

# Beautify the radar chart
ax.set_rlabel_position(30)
ax.tick_params(colors='darkblue', labelsize=18)
ax.spines['polar'].set_visible(True)
ax.spines['polar'].set_color('darkblue')
ax.spines['polar'].set_linewidth(2)

# Set the range of the radar chart with the specified units
ax.set_ylim(0, 0.9)
ax.set_yticks([0, 0.3, 0.6, 0.9])
ax.set_yticklabels(['0', '0.3', '0.6', '0.9'], fontsize=18, color='darkblue')

# # Add a title and legend
# plt.title('Absolute Differences Between Dr. B and Dr. H Across Categories', size=20, color='darkorange', y=1.1)
# ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1), fontsize=12, frameon=True, framealpha=1, shadow=True, borderpad=1)

# Display the radar chart
plt.show()

"""Figure out extreme cases for involving 3rd and 4th graders"""

# Loop through each category and find cases with more than a 1-point difference
extreme_cases = []

for category, (col_h, col_b) in categories.items():
    # Calculate the absolute difference between the two graders
    MDGrade['Difference'] = abs(MDGrade[col_h] - MDGrade[col_b])

    # Filter rows where the difference is greater than 1
    extreme_diff = MDGrade[MDGrade['Difference'] > 1]

    if not extreme_diff.empty:
        extreme_cases.append((category, extreme_diff[[col_h, col_b, 'Difference']]))

# Print out the extreme cases
for category, cases in extreme_cases:
    print(f"\nExtreme cases for {category}:")
    display(cases)

