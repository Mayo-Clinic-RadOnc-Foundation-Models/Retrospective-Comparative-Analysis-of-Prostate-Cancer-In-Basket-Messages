# -*- coding: utf-8 -*-
"""NPJ_Submission_GraderStudy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HwxmC1-VMX_bPcN2Sa-p7uGqA3rlLKD9

## **Installing and loading the relevant libraries**
"""

#Install the libraries
!pip install pandas
!pip install --upgrade openai
!pip install numpy
!pip install transformers
!pip install spacy
!python -m spacy download en_core_web_sm

import pandas as pd
import openai
import matplotlib.pyplot as plt
import spacy
from transformers import pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import r2_score
import nltk
import numpy as np
import scipy.stats as stats
import seaborn as sns

nltk.download('punkt')
from nltk.tokenize import word_tokenize, sent_tokenize

"""## **Grader Study Analysis**

# Four Clinician Graders Analysis

Calculate two clinicians' grading results seperately
"""

# Function to compute statistics for each classification
def compute_statistics(df, columns, classification_column):
    stats = {}
    for category in df[classification_column].unique():
        subset = df[df[classification_column] == category]
        stats[category] = {
            'mean': subset[columns].mean(),
            'std': subset[columns].std()
        }
    return stats

# Compute statistics for 2 doctors based on 'Types'
dr_1_stats = compute_statistics(LLMMDs, dr_1_columns, 'Types')
dr_2_stats = compute_statistics(LLMMDs, dr_2_columns, 'Types')

# Print statistics for Dr. 1
print("Statistics for Dr. 1:")
for category, stat in dr_1_stats.items():
    print(f"\nCategory: {category}")
    for column in dr_1_columns:
        print(f"{column} - Mean: {stat['mean'][column]:.2f}, Std: {stat['std'][column]:.2f}")

# Print statistics for Dr. 2
print("\nStatistics for Dr. 2:")
for category, stat in dr_2_stats.items():
    print(f"\nCategory: {category}")
    for column in dr_2_columns:
        print(f"{column} - Mean: {stat['mean'][column]:.2f}, Std: {stat['std'][column]:.2f}")

# Define column groups, excluding 'Extensive editing required'
group1_columns = ['Dr.1Completeness', 'Dr.1Correctness', 'Dr.1Clarity', 'Dr.1Empathy']
group2_columns = [' DR.2Completeness', ' Dr.2Correctness', 'Dr.2Clarity', 'Dr.2Empathy']

# Check for missing columns
missing_group1 = [col for col in group1_columns if col not in LLMMDs.columns]
missing_group2 = [col for col in group2_columns if col not in LLMMDs.columns]

# Proper error handling with concatenation
if missing_group1 or missing_group2:
    missing_columns_message = (
        f"Missing columns in Group 1: {', '.join(missing_group1)}\n"
        f"Missing columns in Group 2: {', '.join(missing_group2)}"
    )
    raise ValueError(missing_columns_message)

# Convert columns to numeric type, coercing errors to NaN
for col in group1_columns:
    LLMMDs[col] = pd.to_numeric(LLMMDs[col], errors='coerce')
for col in group2_columns:
    LLMMDs[col] = pd.to_numeric(LLMMDs[col], errors='coerce')

# Function to calculate statistics for each classification
def calculate_classification_statistics(df, columns, classification_column):
    stats = {}
    for classification in df[classification_column].unique():
        subset = df[df[classification_column] == classification]
        stats[classification] = subset[columns].agg(['mean', 'std'])
    return stats

# Calculate statistics for each classification in Group 1
group1_stats = calculate_classification_statistics(LLMMDs, group1_columns, 'Types')

# Calculate statistics for each classification in Group 2
group2_stats = calculate_classification_statistics(LLMMDs, group2_columns, 'Types')

# Display the statistics for Group 1
print("Statistics for Group 1:")
for classification, stats in group1_stats.items():
    print(f"\nClassification: {classification}")
    print(stats)

# Display the statistics for Group 2
print("\nStatistics for Group 2:")
for classification, stats in group2_stats.items():
    print(f"\nClassification: {classification}")
    print(stats)

# Separate the data into 'GPT' and 'human' groups
gpt_data = LLMMDs[LLMMDs['Types'] == 'GPT']
human_data = LLMMDs[LLMMDs['Types'] == 'human']

# Define the columns of interest
columns = ['Dr.1Total', 'Dr.2Total', 'LLMTotal']

# Calculate descriptive statistics for 'human' group
human_stats = human_data[columns].describe()

# Calculate descriptive statistics for 'GPT' group
gpt_stats = gpt_data[columns].describe()

# Display the statistics for both groups
print("Human Statistics:")
print(human_stats)

print("\nGPT Statistics:")
print(gpt_stats)

"""Calculate Two Graders' Differences and Potential Bias"""

# Dictionary to store the Pearson correlation results
pearson_results = {}

# Calculate Pearson correlation for each category
for category, (col_1, col_2) in categories.items():
    # Filter out NaNs and inf values
    filtered_data = MDGrade[[col_1, col_2]].replace([np.inf, -np.inf], np.nan).dropna()

    grader1 = filtered_data[col_1].values
    grader2 = filtered_data[col_2].values

    # Calculate Pearson correlation coefficient
    pearson_corr, _ = pearsonr(grader1, grader2)

    # Store the result
    pearson_results[category] = pearson_corr

# Display the results
for category, corr in pearson_results.items():
    print(f"{category} Pearson correlation coefficient: {corr}")

from scipy.stats import ttest_ind
# Dictionary to store the t-statistics and p-values results
t_test_results = {}

# Calculate t-statistics and p-values for each category
for category, (col_1, col_2) in categories.items():
    # Filter out NaNs and inf values
    filtered_data = MDGrade[[col_1, col_2]].replace([np.inf, -np.inf], np.nan).dropna()

    grader1 = filtered_data[col_1].values
    grader2 = filtered_data[col_2].values

    # Calculate t-statistics and p-value
    t_stat, p_value = ttest_ind(grader1, grader2, equal_var=False)  # Assuming unequal variance (Welch's t-test)

    # Store the result
    t_test_results[category] = {'t-statistic': t_stat, 'p-value': p_value}

# Display the results
for category, results in t_test_results.items():
    print(f"{category} t-statistic: {results['t-statistic']}, p-value: {results['p-value']}")

# Dictionary to store the ICC results
icc_results = {}

# Calculate ICC for each category
for category, (col_1, col_2) in categories.items():
    # Prepare the data in the required format
    data = pd.DataFrame({
        'Target': range(len(MDGrade)),
        'Grader1': MDGrade[col_1],
        'Grader2': MDGrade[col_2]
    })

    # Melt the DataFrame to long format
    data_long = data.melt(id_vars=['Target'], var_name='Rater', value_name='Score')

    # Calculate the ICC, omitting rows with NaN values
    icc = pg.intraclass_corr(data=data_long, targets='Target', raters='Rater', ratings='Score', nan_policy='omit')

    # Extract the ICC value (ICC2k is often used for absolute agreement)
    icc_value = icc.loc[icc['Type'] == 'ICC2k', 'ICC'].values[0]

    # Store the result
    icc_results[category] = icc_value

# Display the ICC results
for category, icc_value in icc_results.items():
    print(f"{category} ICC: {icc_value}")

# Dictionary to store the Cohen's Kappa results
kappa_results = {}

# Calculate Cohen's Kappa score for each category
for category, (col_1, col_2) in categories.items():
    # Drop rows with NaN values
    filtered_data = MDGrade[[col_1, col_2]].dropna()

    # Calculate Cohen's Kappa score
    kappa_score = cohen_kappa_score(filtered_data[col_h], filtered_data[col_b])

    # Store the result
    kappa_results[category] = kappa_score

# Display the Cohen's Kappa results
for category, kappa_score in kappa_results.items():
    print(f"{category} Cohen's Kappa score: {kappa_score}")

# Dictionary to store the linear regression results
regression_results = {}

# Perform linear regression for each category
for category, (col_1, col_2) in categories.items():
    # Drop rows with NaN values
    filtered_data = MDGrade[[col_1, col_2]].dropna()

    # Reshape the data for sklearn
    X = filtered_data[col_1].values.reshape(-1, 1)  # Predictor (Dr. H)
    y = filtered_data[col_2].values  # Response (Dr. B)

    # Perform linear regression
    model = LinearRegression()
    model.fit(X, y)

    # Make predictions
    y_pred = model.predict(X)

    # Calculate R-squared
    r2 = r2_score(y, y_pred)

    # Store the coefficients, intercept, and R-squared
    regression_results[category] = {
        'Coefficient': model.coef_[0],
        'Intercept': model.intercept_,
        'R-squared': r2
    }

# Display the regression results
for category, results in regression_results.items():
    print(f"{category} Linear Regression:")
    print(f"  Coefficient: {results['Coefficient']}")
    print(f"  Intercept: {results['Intercept']}")
    print(f"  R-squared: {results['R-squared']}\n")

# Initialize lists to store mean values for Dr. H and Dr. B
values_1 = []
values_2 = []

# Calculate mean values for each category
for category, (col_1, col_2) in categories.items():
    mean_1 = MDGrade[col_1].mean()
    mean_2 = MDGrade[col_2].mean()
    values_1.append(mean_1)
    values_2.append(mean_2)

values_1, values_2

"""Calculate the difference between the two graders"""

# Define the categories and corresponding columns
categories = {
    'Completeness': ['Dr.1Completeness', ' DR.2Completeness'],
    'Correctness': ['Dr.1Correctness', ' Dr.2Correctness'],
    'Clarity': [' Dr.1Clarity', 'Dr.2Clarity'],
    'Empathy': ['Dr.1Empathy', 'Dr.2Empathy']
}

# Calculate the absolute differences between Dr. H and Dr. B
values_diff = np.abs(np.array([0.4, 0.5, 0.2, 0.3]))  # Replace with your calculated values

# Combine the values with the first element to close the radar chart
values_diff = np.concatenate((values_diff, [values_diff[0]]))

# Calculate the angle for each category on the radar chart
angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
angles += angles[:1]  # Ensure the chart is circular

# Create the radar chart
fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))

# Plot the absolute differences
ax.plot(angles, values_diff, linewidth=2, linestyle='solid', color='darkorange', label='Absolute Difference (|B - H|)')
ax.fill(angles, values_diff, color='orange', alpha=0.4)

# Add the category labels to the chart
ax.set_xticks(angles[:-1])
ax.set_xticklabels(categories.keys(), fontsize=26, color='darkblue')

# Beautify the radar chart
ax.set_rlabel_position(30)
ax.tick_params(colors='darkblue', labelsize=18)
ax.spines['polar'].set_visible(True)
ax.spines['polar'].set_color('darkblue')
ax.spines['polar'].set_linewidth(2)

# Set the range of the radar chart with the specified units
ax.set_ylim(0, 0.9)
ax.set_yticks([0, 0.3, 0.6, 0.9])
ax.set_yticklabels(['0', '0.3', '0.6', '0.9'], fontsize=18, color='darkblue')

# # Add a title and legend
# plt.title('Absolute Differences Between Dr. B and Dr. H Across Categories', size=20, color='darkorange', y=1.1)
# ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1), fontsize=12, frameon=True, framealpha=1, shadow=True, borderpad=1)

# Display the radar chart
plt.show()

# Loop through each category and find cases with more than a 1-point difference
extreme_cases = []

for category, (col_1, col_2) in categories.items():
    # Calculate the absolute difference between the two graders
    MDGrade['Difference'] = abs(MDGrade[col_h] - MDGrade[col_b])

    # Filter rows where the difference is greater than 1
    extreme_diff = MDGrade[MDGrade['Difference'] > 1]

    if not extreme_diff.empty:
        extreme_cases.append((category, extreme_diff[[col_1, col_2, 'Difference']]))

# Print out the extreme cases
for category, cases in extreme_cases:
    print(f"\nExtreme cases for {category}:")
    display(cases)

"""Visualizations for the Grader Study Results"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Define the statistics with consistent naming
group1_stats = {
    'GPT': {
        'Completeness': {'mean': 4.435897, 'std': 0.958355},
        'Correctness': {'mean': 4.294872, 'std': 1.102595},
        'Clarity': {'mean': 4.884615, 'std': 0.507512},
        'Empathy': {'mean': 4.554839, 'std': 0.722028}
    },
    'human': {
        'Completeness': {'mean': 4.532468, 'std': 0.697042},
        'Correctness': {'mean': 4.642857, 'std': 0.701805},
        'Clarity': {'mean': 4.902597, 'std': 0.454023},
        'Empathy': {'mean': 4.435065, 'std': 0.807848}
    }
}

group2_stats = {
    'GPT': {
        'Completeness': {'mean': 4.690323, 'std': 0.734739},
        'Correctness': {'mean': 4.361290, 'std': 0.992726},
        'Clarity': {'mean': 4.929032, 'std': 0.343959},
        'Empathy': {'mean': 4.858065, 'std': 0.368195}
    },
    'human': {
        'Completeness': {'mean': 4.774194, 'std': 0.629867},
        'Correctness': {'mean': 4.741935, 'std': 0.632853},
        'Clarity': {'mean': 4.903226, 'std': 0.407307},
        'Empathy': {'mean': 4.606452, 'std': 0.697833}
    }
}
# Function to calculate average between Dr. H and Dr. B for each category
def calculate_average(group1, group2, category):
    return (group1[category]['mean'] + group2[category]['mean']) / 2

# Define categories
categories = ['Completeness', 'Correctness', 'Clarity', 'Empathy']

# Create data for plotting
data = {'Category': [], 'Classification': [], 'Average': []}

# Calculate averages for GPT
for category in categories:
    data['Category'].append(category)
    data['Classification'].append('GPT')
    data['Average'].append(calculate_average(group1_stats['GPT'], group2_stats['GPT'], category))

# Calculate averages for Human
for category in categories:
    data['Category'].append(category)
    data['Classification'].append('human')
    data['Average'].append(calculate_average(group1_stats['human'], group2_stats['human'], category))

# Convert to DataFrame
df = pd.DataFrame(data)

# Plotting with beautification
plt.figure(figsize=(10, 6))

# Set the style
sns.set(style="whitegrid", font_scale=1.2)

# Create the barplot
ax = sns.barplot(x='Category', y='Average', hue='Classification', data=df, palette={'GPT': '#2196F3', 'human': '#f5a525'})

# Add data labels on the bars
for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 10), textcoords='offset points', fontsize=12)

# Titles and labels
# plt.title('Average Comparison for GPT and Human', fontsize=16, weight='bold')
plt.xlabel('Category', fontsize=18)
plt.ylabel('Average Score', fontsize=18)

# Limit y-axis to start at 4 for better focus
plt.ylim(4, 5)

# Customize legend
import matplotlib.patches as mpatches

# Create legend patches for color boxes
gpt_patch = mpatches.Patch(color='#2196F3', label='RadOnc-GPT')
human_patch = mpatches.Patch(color='#f5a525', label='Human Care Team')

# Modify legend to include the color patches
plt.legend(handles=[gpt_patch, human_patch], title='Response Types', title_fontsize='16', loc='upper right', fontsize=12)
plt.rcParams['figure.dpi'] = 280

# Final layout adjustments
plt.tight_layout()

# Show the plot
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Data for plotting
categories = ['Completeness', 'Correctness', 'Clarity', 'Empathy']

# Mean scores for each group and category
group1_means = {
    'GPT': [4.435897, 4.294872, 4.884615, 4.554839],
    'human': [4.532468, 4.642857, 4.902597, 4.435065]
}

group2_means = {
    'GPT': [4.690323, 4.361290, 4.929032, 4.858065],
    'human': [4.774194, 4.741935, 4.903226, 4.606452]
}

# Set up the figure and axis
fig, ax = plt.subplots(figsize=(12, 7))

# Define bar width and position
bar_width = 0.2
x = np.arange(len(categories))  # the label locations

# Plot bars for Group 1
bars1 = ax.bar(x - 1.5 * bar_width, group1_means['GPT'], bar_width, label='RadOnc-GPT (Grader 1)', color='#2196F3')
bars2 = ax.bar(x - 0.5 * bar_width, group1_means['human'], bar_width, label='Human Care Team (Grader 1)', color='#f5a525')

# Plot bars for Group 2 with similar but slightly different colors
bars3 = ax.bar(x + 0.5 * bar_width, group2_means['GPT'], bar_width, label='RadOnc-GPT (Grader 2)', color='#1976D2')  # Slightly darker tone
bars4 = ax.bar(x + 1.5 * bar_width, group2_means['human'], bar_width, label='Human Care Team (Grader 2)', color='#F57C00')  # Slightly darker tone

# Add some text for labels, title, and custom x-axis tick labels
ax.set_xlabel('Categories', fontsize=18)
ax.set_ylabel('Mean Scores', fontsize=18)
ax.set_xticks(x)
ax.set_xticklabels(categories, fontsize=16)

# Add data labels
def add_labels(bars):
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2, height, f'{height:.2f}', ha='center', va='bottom', fontsize=10)

add_labels(bars1)
add_labels(bars2)
add_labels(bars3)
add_labels(bars4)

# Add legend and display the plot
ax.legend(loc='lower right', fontsize=12)
# plt.title('Mean Scores by Category for GPT and Human Care Team Across Two Groups', fontsize=16)
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Set figure DPI and size
plt.figure(figsize=(12, 6), dpi=180)

# Create a violin plot
sns.violinplot(x='Total Type', y='Total Value', hue='Category', data=combined_totals,
               palette={'Human Care Team': '#f5a525', 'Grader RadOnc-GPT': '#2196F3'}, split=True,
               inner="box", linewidth=2.5)  # Use correct palette colors

# Update the X-axis labels (renaming 'HTotal' -> 'Grader 1', 'BTotal' -> 'Grader 2')
new_labels = ['Grader 1', 'Grader 2']
plt.xticks(ticks=[0, 1], labels=new_labels, fontsize=18)

# Update the X-axis label
plt.xlabel('Graders', fontsize=18)

# Fix the legend issue by explicitly setting the color and title
handles, labels = plt.gca().get_legend_handles_labels()
plt.legend(handles=handles, labels=['Human Care Team', 'RadOnc-GPT'], title='In-Basket Response Type',
           fontsize=12, title_fontsize='13', loc='lower right')

# Add a title and adjust gridlines
plt.title('Comparison of Two Clinician Graders and RadOnc-GPT (Violin Plot)', fontsize=20)

# Show the plot
plt.tight_layout()
plt.show()

# Separate the data into 'GPT' and 'human' based on 'random number'
gpt_data = LLMMDs[LLMMDs['random number'] == 'GPT']
human_data = LLMMDs[LLMMDs['random number'] == 'human']

# Categories to classify (removing 'Extensive Editing Required')
categories = [
    'Will use this without editing',
    'Minor (<1 minute) editing',
    'Major editing',
    'Would not use this'
]

# Count occurrences for each grader for GPT and human
gpt_dr_h_counts = gpt_data['Dr.HExtensive editing required'].value_counts().reindex(categories, fill_value=0)
gpt_general_counts = gpt_data['Extensive editing required'].value_counts().reindex(categories, fill_value=0)
human_dr_h_counts = human_data['Dr.HExtensive editing required'].value_counts().reindex(categories, fill_value=0)
human_general_counts = human_data['Extensive editing required'].value_counts().reindex(categories, fill_value=0)

# Take the average of the two graders for GPT and human
gpt_avg_counts = (gpt_dr_h_counts + gpt_general_counts) / 2
human_avg_counts = (human_dr_h_counts + human_general_counts) / 2

# Combine the averages into a single DataFrame
comparison_avg_df = pd.DataFrame({
    'GPT (Average)': gpt_avg_counts,
    'Human (Average)': human_avg_counts
})

# Plot the comparison of averages
ax = comparison_avg_df.plot(kind='bar', figsize=(12, 7), color=['#2196F3', '#f5a525'])
# plt.title('Comparison of Editing Requirements (Averaged: GPT vs Human)')
plt.xlabel('Editing Requirement Type', fontsize = 18)
plt.ylabel('Average Count', fontsize = 18)
plt.xticks(rotation=30)
plt.legend(title='Grader Type', labels=['RadOnc-GPT Average', 'Human Care Team Average'])

# Add data labels in the middle of each top of the bar
for p in ax.patches:
    height = p.get_height()
    if height > 0:
        ax.annotate(f'{height:.1f}',
                    (p.get_x() + p.get_width() / 2, height),  # Place label at middle top of the bar
                    ha='center',
                    va='bottom',  # Align text at the top of the bar
                    fontsize=13)

plt.tight_layout()
plt.show()
plt.rcParams['figure.dpi'] = 280

"""Summarize and Download the responses which require major editing or no use"""

# Replace with your actual filtered DataFrames
# For GPT filtered data
gpt_filtered = pd.DataFrame({
    'random number': ['GPT', 'GPT'],
    'Dr.HExtensive editing required': ['Major editing', 'Would not use this'],
    'Extensive editing required': ['Major editing', 'Would not use this']
})

# For Human filtered data
human_filtered = pd.DataFrame({
    'random number': ['human', 'human'],
    'Dr.HExtensive editing required': ['Major editing', 'Would not use this'],
    'Extensive editing required': ['Major editing', 'Would not use this']
})

# Save to an Excel file
file_name = 'filtered_major_editing_and_would_not_use.xlsx'
with pd.ExcelWriter(file_name) as writer:
    gpt_filtered.to_excel(writer, sheet_name='GPT_Filtered', index=False)
    human_filtered.to_excel(writer, sheet_name='Human_Filtered', index=False)

"""Three Graders Correlation"""

# Completeness comparison
MDGrade['CompletenessAverage'] = MDGrade[['Dr.1Completeness', ' DR.2Completeness', 'Dr.3completeness']].mean(axis=1)

# Correctness comparison
MDGrade['CorrectnessAverage'] = MDGrade[['Dr.1Correctness', ' Dr.2Correctness', 'Dr.3correctness']].mean(axis=1)

# Clarity comparison
MDGrade['ClarityAverage'] = MDGrade[[' Dr.1Clarity', 'Dr.2Clarity', 'Dr.3clarity']].mean(axis=1)

# Empathy comparison
MDGrade['EmpathyAverage'] = MDGrade[['Dr.1Empathy', 'Dr.2Empathy', 'Dr.3empathy']].mean(axis=1)

# Completeness correlation
completeness_corr = MDGrade[['Dr.1Completeness', ' DR.2Completeness', 'Dr.3completeness']].corr()

# Correctness correlation
correctness_corr = MDGrade[['Dr.1HCorrectness', ' Dr.2Correctness', 'Dr.3correctness']].corr()

# Clarity correlation
clarity_corr = MDGrade[[' Dr.1Clarity', 'Dr.2Clarity', 'Dr.3clarity']].corr()

# Empathy correlation
empathy_corr = MDGrade[['Dr.1Empathy', 'Dr.2Empathy', 'Dr.3empathy']].corr()

print(completeness_corr, correctness_corr, clarity_corr, empathy_corr)

# Combine the correlation matrices into a single DataFrame
combined_corr = pd.concat([completeness_corr, correctness_corr, clarity_corr, empathy_corr], axis=1)

# Rename the index similarly for readability
combined_corr.index = ['Dr.1 Completeness', 'Dr.2 Completeness', 'Dr.3Completeness',
                       'Dr.1 Correctness', 'Dr.2 Correctness', 'Dr.3Correctness',
                       'Dr.1 Clarity', 'Dr.2 Clarity', 'Dr.3Clarity',
                       'Dr.1 Empathy', 'Dr.2 Empathy', 'Dr.3Empathy']

# Plot the combined heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(combined_corr, annot=True, cmap="coolwarm", vmin=-1, vmax=1)
plt.title("Combined Heatmap of Graders' Correlation Across Categories")
plt.show()

# After cleaning the data
f_value_completeness, p_value_completeness = f_oneway(MDGrade_cleaned['Dr.1Completeness'], MDGrade_cleaned[' DR.2Completeness'], MDGrade_cleaned['Dr.3completeness'])
f_value_correctness, p_value_correctness = f_oneway(MDGrade_cleaned['Dr.1Correctness'], MDGrade_cleaned[' Dr.2Correctness'], MDGrade_cleaned['Dr.3correctness'])
f_value_clarity, p_value_clarity = f_oneway(MDGrade_cleaned[' Dr.1Clarity'], MDGrade_cleaned['Dr.2Clarity'], MDGrade_cleaned['Dr.3clarity'])
f_value_empathy, p_value_empathy = f_oneway(MDGrade_cleaned['Dr.1Empathy'], MDGrade_cleaned['Dr.2Empathy'], MDGrade_cleaned['Dr.3empathy'])

# Step 4: Output the F-values for each category
print("F-values:")
print(f"Completeness: F-value = {f_value_completeness}, p-value = {p_value_completeness}")
print(f"Correctness: F-value = {f_value_correctness}, p-value = {p_value_correctness}")
print(f"Clarity: F-value = {f_value_clarity}, p-value = {p_value_clarity}")
print(f"Empathy: F-value = {f_value_empathy}, p-value = {p_value_empathy}")

# Descriptive statistics for the variability columns
variability_stats = ThreeGrader[['Completeness_Variability', 'Correctness_Variability', 'Clarity_Variability', 'Empathy_Variability']].describe()

# Display the variability statistics
print(variability_stats)

extreme_completeness = ThreeGrader.nlargest(1, 'Completeness_Variability')
extreme_correctness = ThreeGrader.nlargest(1, 'Correctness_Variability')
extreme_clarity = ThreeGrader.nlargest(1, 'Clarity_Variability')
extreme_empathy = ThreeGrader.nlargest(1, 'Empathy_Variability')
# extreme_extensive_editing = ThreeGrader.nlargest(1, 'Extensive_Editing_Variability')

# Display the extreme cases
print("Extreme Case for Completeness Variability:")
print(extreme_completeness)

print("Extreme Case for Correctness Variability:")
print(extreme_correctness)

print("Extreme Case for Clarity Variability:")
print(extreme_clarity)

print("Extreme Case for Empathy Variability:")
print(extreme_empathy)

# print("Extreme Case for Extensive Editing Required Variability:")
# print(extreme_extensive_editing)