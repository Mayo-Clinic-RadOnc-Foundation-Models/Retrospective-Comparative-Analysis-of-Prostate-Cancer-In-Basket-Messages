# -*- coding: utf-8 -*-
"""NPJ_Submission_NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P3rNMDJI_JtKLAU_o4cJUNgjpEzVdAiT

## **Installing and loading the relevant libraries**
"""

#Install the libraries
!pip install pandas
!pip install --upgrade openai
!pip install numpy
!pip install transformers
!pip install spacy
!python -m spacy download en_core_web_sm
!pip install vaderSentiment

import pandas as pd
import openai
import matplotlib.pyplot as plt
import spacy
from transformers import pipeline
import nltk
import seaborn as sns
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import numpy as np
import scipy.stats as stats
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch
import textstat

nltk.download('punkt')
from nltk.tokenize import word_tokenize, sent_tokenize

"""## **Nature Language Processing (NLP) Analysis**

Sentiment Analysis
"""

# Load spaCy model for aspect extraction
nlp = spacy.load('en_core_web_sm')

# Load sentiment analysis pipeline from transformers
sentiment_pipeline = pipeline("sentiment-analysis")

def extract_aspects(text):
    doc = nlp(text)
    aspects = set()
    for ent in doc.ents:
        aspects.add(ent.text)
    return list(aspects)

def analyze_sentiment(text):
    result = sentiment_pipeline(text)
    return result[0]['label']

def aspect_based_sentiment_analysis(text):
    aspects = extract_aspects(text)
    sentiment_results = {}
    for aspect in aspects:
        aspect_sentences = [sent for sent in text.split('.') if aspect in sent]
        aspect_sentiments = [analyze_sentiment(sentence) for sentence in aspect_sentences]
        sentiment_results[aspect] = aspect_sentiments
    return sentiment_results

"""TextBlob Analysis"""

# Now let's calculate sentiment scores for both columns
def get_sentiment(text):
    analysis = TextBlob(str(text))
    # Polarity is a float within the range [-1.0, 1.0] where 1.0 means positive statement and -1.0 means a negative statement.
    return analysis.sentiment.polarity

# Apply the sentiment function to both columns
data['GPT_Sentiment'] = data['GPT Respond'].apply(get_sentiment)
data['CareTeam_Sentiment'] = data['Clinician Respond'].apply(get_sentiment)

# Let's compare the sentiment scores
# Calculating the average sentiment for both
average_sentiment = data[['GPT_Sentiment', 'CareTeam_Sentiment']].mean()

# Showing the average sentiment
print("Average Sentiment Scores:")
print(average_sentiment)

# Display the updated dataframe with sentiment scores
data.head()

plt.rcParams['figure.dpi'] = 380

plt.hist(data['GPT_Sentiment'], alpha=0.9, color = '#2196F3', label='RadOnc-GPT Sentiment')
plt.hist(data['CareTeam_Sentiment'], alpha=0.9, color = '#f5a525', label='Care Team Sentiment')
plt.xlabel('Sentiment Score', fontsize=18)  # Increased font size for x-axis label
plt.ylabel('Frequency', fontsize=18)  # Increased font size for y-axis label
plt.title('TextBlob Sentiment Distribution', fontsize=20)  # Increased font size for title

# Adding the trend indicators
plt.axvline(-1, color='red', linestyle='--', label='Very Negative')
plt.axvline(0, color='gray', linestyle='--', label='Neutral')
plt.axvline(1, color='green', linestyle='--', label='Very Positive')

_ = plt.legend()
plt.show()

"""VADER Analysis"""

# Initialize VADER sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# Function to calculate VADER sentiment
def get_vader_sentiment(text):
    vs = analyzer.polarity_scores(str(text))
    return vs['compound']

# Apply VADER sentiment to both columns
data['GPT_VADER_Sentiment'] = data['GPT Respond'].apply(get_vader_sentiment)
data['CareTeam_VADER_Sentiment'] = data['Clinician Respond'].apply(get_vader_sentiment)

plt.rcParams['figure.dpi'] = 380

plt.figure(figsize=(8, 6))
sns.boxplot(data=[data['GPT_VADER_Sentiment'], data['CareTeam_VADER_Sentiment']],
            palette=['#2196F3', '#f5a525'])
plt.xticks([0, 1], ['RadOnc-GPT', 'Human Care Team'], fontsize=18)  # Increased font size for x-axis labels
plt.ylabel('Compound Sentiment Score', fontsize=18)  # Increased font size for y-axis label
plt.title('VADER Sentiment Distribution', fontsize=20)  # Increased font size for title
plt.show()

"""Quantitative Words and Sentences"""

# Step 3: Define Functions to Calculate Words and Sentences
def count_words(text):
    return len(word_tokenize(text))

def count_sentences(text):
    return len(sent_tokenize(text))

# Step 4: Apply the Functions to Each Column
# Ensure that text data is treated as a string
data['MessageNote'] = data['MessageNote'].astype(str)
data['GPT Respond'] = data['GPT Respond'].astype(str)
data['Clinician Respond'] = data['Clinician Respond'].astype(str)

# Calculate words and sentences
data['MessageNote_Words'] = data['MessageNote'].apply(count_words)
data['MessageNote_Sentences'] = data['MessageNote'].apply(count_sentences)

data['GPT_Words'] = data['GPT Respond'].apply(count_words)
data['GPT_Sentences'] = data['GPT Respond'].apply(count_sentences)

data['Clinician_Words'] = data['Clinician Respond'].apply(count_words)
data['Clinician_Sentences'] = data['Clinician Respond'].apply(count_sentences)

# Define a function to calculate 95% confidence interval
def confidence_interval(data, confidence=0.95):
    n = len(data)
    mean = np.mean(data)
    stderr = stats.sem(data)
    h = stderr * stats.t.ppf((1 + confidence) / 2, n - 1)
    return mean - h, mean + h

# Calculate statistics for each category
stats_dict = {}

for category in ['MessageNote', 'GPT', 'Clinician']:
    stats_dict[f'{category}_Words'] = {
        'mean': data[f'{category}_Words'].mean(),
        'median': data[f'{category}_Words'].median(),
        'std_dev': data[f'{category}_Words'].std(),
        '95%_CI': confidence_interval(data[f'{category}_Words'])
    }
    stats_dict[f'{category}_Sentences'] = {
        'mean': data[f'{category}_Sentences'].mean(),
        'median': data[f'{category}_Sentences'].median(),
        'std_dev': data[f'{category}_Sentences'].std(),
        '95%_CI': confidence_interval(data[f'{category}_Sentences'])
    }

# Display the results
for key, value in stats_dict.items():
    print(f"{key} - Mean: {value['mean']:.2f}, Median: {value['median']:.2f}, "
          f"Std Dev: {value['std_dev']:.2f}, 95% CI: {value['95%_CI']}")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Update the legend wordings in the 'Category' column
filtered_data['Category'] = filtered_data['Category'].replace({'GPT Responses': 'RadOnc-GPT', 'Clinician Responses': 'Human Care Team'})

# Create a scatter plot with regression lines for GPT and Clinician only
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Words', y='Sentences', hue='Category', data=filtered_data, palette=['#2196F3', '#f5a525'], s=40)
sns.regplot(x='Words', y='Sentences', data=filtered_data[filtered_data['Category'] == 'RadOnc-GPT'], scatter=False, color='#2196F3', line_kws={"linewidth":2})
sns.regplot(x='Words', y='Sentences', data=filtered_data[filtered_data['Category'] == 'Human Care Team'], scatter=False, color='#f5a525', line_kws={"linewidth":2})

# Set the axis limits to zoom in on the cluster
plt.xlim(0, 400)
plt.ylim(0, 20)

# Add titles and labels
# plt.title('Comparative Analysis of Word and Sentence Counts', fontsize=20)
plt.xlabel('Word Count', fontsize=18)
plt.ylabel('Sentence Count', fontsize=18)

# Improve legend
plt.legend(title='Response', title_fontsize='18', fontsize='16')

# Show the plot
plt.tight_layout()
plt.show()

"""Natural Language Inference (NLI)"""

import matplotlib.pyplot as plt

# Data for the graph
nli_labels = ['Neutral', 'Entailment', 'Contradiction']
clinician_proportions = [70.253165, 29.113924, 0.632911]
gpt_proportions = [92.405063, 4.430380, 3.164557]

# Create the bar plot
x = range(len(nli_labels))
width = 0.35

fig, ax = plt.subplots()
rects1 = ax.bar(x, clinician_proportions, width, color='#f5a525', label='Human Care Team Respond')
rects2 = ax.bar([i + width for i in x], gpt_proportions, width, color='#2196F3', label='RadOnc-GPT Respond')

# Add labels and title
ax.set_ylabel('Proportion (%)', fontsize=18)
ax.set_xlabel('NLI Labels', fontsize=18)
ax.set_title('NLI Label Distribution', fontsize=20)
ax.set_xticks([i + width / 2 for i in x])
ax.set_xticklabels(nli_labels, fontsize=14)
ax.legend()

# Add values on top of the bars with improved positioning
def autolabel(rects, ax, offset=0):
    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2., height - offset,  # Position the text slightly lower
                '%0.1f' % float(height),
                ha='center', va='bottom', fontsize=12)

# Adjust the offset for the GPT bar to move the label lower
autolabel(rects1, ax, offset=0)  # No offset for Clinician bars
autolabel(rects2, ax, offset=3)  # Apply an offset of 3 units for GPT bars

plt.rcParams['figure.dpi'] = 180

plt.show()

# Load the pre-trained model and tokenizer
model_name = "roberta-large-mnli"  # You can also try "bert-base-mnli"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Function to perform NLI
def perform_nli(premise, hypothesis):
    inputs = tokenizer.encode_plus(premise, hypothesis, return_tensors='pt', truncation=True)
    outputs = model(**inputs)
    logits = outputs.logits
    predicted_class = torch.argmax(logits, dim=1).item()

    # Map the prediction to the NLI label
    labels = ["entailment", "neutral", "contradiction"]
    return labels[predicted_class]

# Remove the limitation to the first 10 rows
# Apply NLI on each row to calculate two NLI scores
nli_results_clinician = []
nli_results_gpt = []

for i, row in data.iterrows():
    message_note = row['MessageNote']
    clinician_response = row['Clinician Respond']
    gpt_response = row['GPT Respond']

    # NLI between 'MessageNote' and 'Clinician Respond'
    nli_label_clinician = perform_nli(message_note, clinician_response)
    nli_results_clinician.append(nli_label_clinician)

    # NLI between 'MessageNote' and 'GPT Respond'
    nli_label_gpt = perform_nli(message_note, gpt_response)
    nli_results_gpt.append(nli_label_gpt)

# Add the NLI results to the DataFrame
data['NLI_Clinician'] = nli_results_clinician
data['NLI_GPT'] = nli_results_gpt

# Save the DataFrame to an Excel file
file_name = 'nli_results_all_rows.xlsx'
data.to_excel(file_name, index=False)

# Download the Excel file
files.download(file_name)

# Print a sample of the results (optional)
print(data[['MessageNote', 'Clinician Respond', 'GPT Respond', 'NLI_Clinician', 'NLI_GPT']].head())

# Calculate distribution of NLI labels for Clinician Respond
clinician_distribution = data['NLI_Clinician'].value_counts(normalize=True) * 100
gpt_distribution = data['NLI_GPT'].value_counts(normalize=True) * 100

# Print the distributions
print("NLI Distribution for Clinician Respond:")
print(clinician_distribution)

print("\nNLI Distribution for GPT Respond:")
print(gpt_distribution)

# NLI Distribution for Clinician Respond
clinician_distribution = {
    'NLI_Clinician': ['neutral', 'entailment', 'contradiction'],
    'Proportion (%)': [70.253165, 29.113924, 0.632911]
}

# NLI Distribution for GPT Respond
gpt_distribution = {
    'NLI_GPT': ['neutral', 'entailment', 'contradiction'],
    'Proportion (%)': [92.405063, 4.430380, 3.164557]
}

# Create DataFrames
df_clinician = pd.DataFrame(clinician_distribution)
df_gpt = pd.DataFrame(gpt_distribution)

# Combine both DataFrames into a single table
df_combined = pd.DataFrame({
    'NLI_Label': ['neutral', 'entailment', 'contradiction'],
    'Clinician Respond (%)': df_clinician['Proportion (%)'],
    'GPT Respond (%)': df_gpt['Proportion (%)']
})

# Display the table
df_combined

# Generate a graph to illustrate the differences
# Data for the graph
nli_labels = ['neutral', 'entailment', 'contradiction']
clinician_proportions = [70.253165, 29.113924, 0.632911]
gpt_proportions = [92.405063, 4.430380, 3.164557]

# Create the bar plot
x = range(len(nli_labels))
width = 0.35

fig, ax = plt.subplots()
rects1 = ax.bar(x, clinician_proportions, width, color='#f5a525', label='Human Care Team Respond')
rects2 = ax.bar([i + width for i in x], gpt_proportions, width, color='#2196F3', label='RadOnc-GPT Respond')

# Add labels and title
ax.set_ylabel('Proportion (%)', fontsize=18)
ax.set_title('NLI Label Distribution', fontsize=20)
ax.set_xticks([i + width / 2 for i in x])
ax.set_xticklabels(nli_labels)
ax.legend()

# Add values on top of the bars
def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,
                '%0.1f' % float(height),
        ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)
plt.rcParams['figure.dpi'] = 180

plt.show()

"""Similarity Between Responses"""

import numpy as np
from scipy import stats
from sklearn.metrics.pairwise import cosine_similarity

# Assuming gpt_embeddings and clinician_embeddings are already calculated
# Each should be a list or numpy array of embeddings corresponding to each row in the columns

# Step 1: Calculate similarity for each corresponding pair of rows
row_wise_similarities = []

for i in range(len(gpt_embeddings)):
    similarity = cosine_similarity(gpt_embeddings[i].reshape(1, -1),
                                    clinician_embeddings[i].reshape(1, -1))[0][0]
    row_wise_similarities.append(similarity)

# Convert to a numpy array for easier calculations
similarity_array = np.array(row_wise_similarities)

# Step 2: Calculate various statistics
mean_similarity = np.mean(similarity_array)
median_similarity = np.median(similarity_array)
std_dev_similarity = np.std(similarity_array)
range_similarity = np.ptp(similarity_array)  # ptp = "peak to peak"
iqr_similarity = np.percentile(similarity_array, 75) - np.percentile(similarity_array, 25)
skewness_similarity = stats.skew(similarity_array)
kurtosis_similarity = stats.kurtosis(similarity_array)
min_similarity = np.min(similarity_array)
max_similarity = np.max(similarity_array)
confidence_interval = stats.norm.interval(0.95, loc=mean_similarity, scale=stats.sem(similarity_array))

# Print the results
print(f"Mean Similarity: {mean_similarity:.4f}")
print(f"Median Similarity: {median_similarity:.4f}")
print(f"Standard Deviation: {std_dev_similarity:.4f}")
print(f"Range: {range_similarity:.4f}")
print(f"Interquartile Range (IQR): {iqr_similarity:.4f}")
print(f"Skewness: {skewness_similarity:.4f}")
print(f"Kurtosis: {kurtosis_similarity:.4f}")
print(f"Minimum Similarity: {min_similarity:.4f}")
print(f"Maximum Similarity: {max_similarity:.4f}")
print(f"95% Confidence Interval: ({confidence_interval[0]:.4f}, {confidence_interval[1]:.4f})")

# Assuming gpt_embeddings and clinician_embeddings are already calculated
# Each should be a list of embeddings corresponding to each row in the columns

# Step 1: Calculate similarity for each corresponding pair of rows
row_wise_similarities = []

for i in range(len(gpt_embeddings)):
    # Calculate cosine similarity between the corresponding row embeddings
    similarity = cosine_similarity(gpt_embeddings[i].reshape(1, -1),
                                    clinician_embeddings[i].reshape(1, -1))[0][0]
    row_wise_similarities.append(similarity)

# Convert to a DataFrame for easier handling
similarity_df = pd.DataFrame({
    'Row_Index': range(len(row_wise_similarities)),
    'Similarity': row_wise_similarities
})

# Save the DataFrame to an Excel file
file_name = 'semantic_similarity_scores.xlsx'
similarity_df.to_excel(file_name, index=False)

# Download the Excel file
files.download(file_name)

# Print a sample of the results (optional)
print(similarity_df.head())

"""Readability Calculations"""

# Function to calculate Flesch Reading Ease score
def calculate_flesch_reading_ease(text):
    return textstat.flesch_reading_ease(text)

# Apply the Flesch Reading Ease calculation to both columns
data['GPT_Flesch_Reading_Ease'] = data['GPT Respond'].apply(lambda x: calculate_flesch_reading_ease(str(x)))
data['Clinician_Flesch_Reading_Ease'] = data['Clinician Respond'].apply(lambda x: calculate_flesch_reading_ease(str(x)))

# Display the DataFrame with the new columns
print(data[['GPT Respond', 'GPT_Flesch_Reading_Ease', 'Clinician Respond', 'Clinician_Flesch_Reading_Ease']].head())

# Calculate summary statistics for GPT and Clinician responses
summary_stats = data[['GPT_Flesch_Reading_Ease', 'Clinician_Flesch_Reading_Ease']].describe()
print(summary_stats)

# Define a function to calculate multiple readability scores
def calculate_readability_scores(text):
    return {
        'Flesch Reading Ease': textstat.flesch_reading_ease(text),
        'Flesch-Kincaid Grade Level': textstat.flesch_kincaid_grade(text),
        'Gunning Fog Index': textstat.gunning_fog(text),
        'SMOG Index': textstat.smog_index(text),
        'Automated Readability Index': textstat.automated_readability_index(text),
        'Coleman-Liau Index': textstat.coleman_liau_index(text),
    }

# Apply the readability calculations to both GPT and Clinician responses
data['MessageNote_Readability'] = data['MessageNote'].apply(lambda x: calculate_readability_scores(str(x)))
data['GPT_Readability'] = data['GPT Respond'].apply(lambda x: calculate_readability_scores(str(x)))
data['Clinician_Readability'] = data['Clinician Respond'].apply(lambda x: calculate_readability_scores(str(x)))

# Create separate columns for each readability test
for metric in data['GPT_Readability'][0].keys():
    data[f'MessageNote_{metric}'] = data['MessageNote_Readability'].apply(lambda x: x[metric])
    data[f'GPT_{metric}'] = data['GPT_Readability'].apply(lambda x: x[metric])
    data[f'Clinician_{metric}'] = data['Clinician_Readability'].apply(lambda x: x[metric])

# Display the DataFrame with the new readability columns
print(data.head())

import numpy as np
import matplotlib.pyplot as plt

# Assuming 'data' is a DataFrame that has been previously defined and contains the required data.

# Calculate the average readability scores across all metrics
messagenote_avg_scores = data[[f'MessageNote_{metric}' for metric in data['MessageNote_Readability'][0].keys()]].mean()
gpt_avg_scores = data[[f'GPT_{metric}' for metric in data['GPT_Readability'][0].keys()]].mean()
clinician_avg_scores = data[[f'Clinician_{metric}' for metric in data['GPT_Readability'][0].keys()]].mean()

# Strip "GPT_" prefix from the metric names for the x-axis
metrics = [metric.replace('GPT_', '') for metric in gpt_avg_scores.index]

# Create a comparison bar plot
plt.figure(figsize=(14, 8))
index = np.arange(len(gpt_avg_scores))
bar_width = 0.25  # Adjusted to fit three bars

# # Plot bars for Message Note, GPT, and Clinician scores
# bars1 = plt.bar(index - bar_width, messagenote_avg_scores, bar_width, label='Patient Inquiry', color='#63E398')
bars2 = plt.bar(index, gpt_avg_scores, bar_width, label='RadOnc-GPT', color='#2196F3')
bars3 = plt.bar(index + bar_width, clinician_avg_scores, bar_width, label='Human Care Team', color='#f5a525')

# Set the labels and title
plt.ylabel('Average Score', fontsize=18)
plt.title('Readability Scores', fontsize=20)
plt.xticks(index, metrics, fontsize=18, rotation=30)

# Adjust the legend to increase marker size and font size
plt.legend(fontsize=18, markerscale=1.9)  # Increased fontsize and markerscale

plt.rcParams['figure.dpi'] = 380

# Add data labels on each bar
for bars in [bars2, bars3]:
    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.1, round(yval, 2), ha='center', va='bottom', fontsize=14, color='black')

plt.tight_layout()
plt.show()